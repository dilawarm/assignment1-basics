{
  "train_data_path": "data/encoded/tinystories_train_tokens.npy",
  "val_data_path": "data/encoded/tinystories_valid_tokens.npy",
  "vocab_size": 10000,
  "context_length": 256,
  "d_model": 512,
  "num_layers": 4,
  "num_heads": 16,
  "d_ff": 1344,
  "rope_theta": 10000.0,
  "eps": 1e-05,
  "max_steps": 12800,
  "batch_size": 64,
  "gradient_accumulation_steps": 1,
  "learning_rate": 3e-4,
  "min_learning_rate": 3e-5,
  "warmup_steps": 1280,
  "weight_decay": 0.1,
  "beta1": 0.9,
  "beta2": 0.95,
  "grad_clip_norm": 1.0,
  "use_amp": true,
  "use_gradient_checkpointing": true,
  "gradient_checkpointing_layers": 3,
  "use_tf32": true,
  "compile_model": true,
  "channels_last": false,
  "use_fused_adamw": true,
  "num_workers": 4,
  "pin_memory": true,
  "prefetch_factor": 2,
  "log_interval": 50,
  "eval_interval": 640,
  "eval_batches": 50,
  "save_interval": 3200,
  "checkpoint_dir": "checkpoints",
  "experiment_name": "tinystories_optimized",
  "experiment_description": "TinyStories training with FlashAttention, AMP, and gradient checkpointing",
  "use_wandb": true,
  "wandb_project": "cs336-assignment1",
  "device": "cuda",
  "resume_from": null,
  "auto_resume": true
}