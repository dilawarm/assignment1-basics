{
  "training_mode": "fast",
  "experiment_name": "h100_optimal_training",
  "experiment_description": "Optimal H100 80GB configuration to beat val_loss 3.0781 in 1.5h with maximum stability and MFU",
  
  "vocab_size": 32000,
  "context_length": 1024,
  "num_layers": 18,
  "d_model": 1536,
  "num_heads": 24,
  "d_ff": 6144,
  "rope_theta": 10000.0,
  "window_size": 512,
  "use_qk_norm": true,
  "use_flex_attention": false,
  "use_swiglu": false,
  "tie_embeddings": false,
  
  "weight_decay": 0.1,
  "betas": [0.9, 0.95],
  "muon_momentum": 0.95,
  "eps": 1e-8,
  
  "max_learning_rate": 0.003,
  "min_learning_rate": 0.0003,
  "warmup_iters": 300,
  "cosine_cycle_iters": 5400,
  
  "training_set": "data/encoded/owt_train_tokens.npy",
  "validation_set": "data/encoded/owt_valid_tokens.npy",
  
  "validation_step_interval": 100,
  "checkpoint_step_interval": 1000,
  "steps": 5400,
  "batch_size": 32,
  "gradient_accumulation_steps": 4,
  "gradient_clipping": 1.0,
  
  "device": "cuda",
  "compile_model": true,
  "compile_mode": "max-autotune",
  "use_mixed_precision": true,
  "use_efficient_attention": true,
  "use_fused_kernels": true,
  
  "enable_stability_monitoring": true,
  "gradient_norm_threshold": 15.0,
  "loss_spike_threshold": 4.0,
  "nan_tolerance": 3,
  "enable_stable_initialization": true,
  "enable_torch_optimizations": true,
  "eval_batch_count": 30,
  
  "use_activation_checkpointing": true,
  "checkpoint_pattern": "layers\\.[0-9]+$",
  "use_memory_efficient_attention": true,
  "optimize_memory_layout": true,
  
  "use_wandb": true,
  "wandb_project": "cs336-assignment1",
  "wandb_entity": "",
  "log_dir": "experiments"
} 